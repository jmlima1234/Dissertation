
\begin{document}

\section*{Chapter 3: Methodology}

\subsection*{3.1 Introduction}

This chapter details the systematic methodology employed to address the challenge of address data quality within the OpenStreetMap (OSM) dataset for Portugal. The approach is rooted in a data-driven pipeline, commencing with the acquisition and preparation of a large-scale geographic dataset, followed by a comprehensive Exploratory Data Analysis (EDA) to quantitatively define the scope of data quality issues. Subsequently, this chapter describes the design and implementation of a bespoke normalization module engineered to resolve the identified inconsistencies. Finally, it presents the validation framework used to empirically measure the effectiveness of this module, thereby providing a robust, reproducible, and transparent account of the methods developed for this thesis \cite{Master_s_Thesis.pdf}.

\subsection*{3.2 Data Acquisition and Preparation}

The foundation of this research is a comprehensive dataset of geographic information for Portugal, sourced from OpenStreetMap (OSM), a global, collaboratively created free editable map \cite{Master_s_Thesis.pdf}. OSM was selected due to its extensive coverage, richness of semantic detail, and open-access nature, which align with the project's goals of creating a reproducible methodology. The data extract for Portugal was obtained from Geofabrik, a reputable distributor of OSM data.

\subsubsection*{3.2.1 Technical Data Processing Pipeline}
To manage the considerable size of the dataset (\texttt{portugal-250902.osm.pbf}, approximately 250MB compressed), a sophisticated data processing pipeline was implemented:

\begin{itemize}
    \item \textbf{Format Selection and Optimization}: The data was acquired in the Protocolbuffer Binary Format (\texttt{.osm.pbf}). This format offers significant performance advantages over traditional XML-based formats due to its smaller file size (approximately 10x compression ratio) and optimized structure for parsing.
    
    \item \textbf{Streaming Data Processing}: The initial data ingestion was performed using a custom Python script leveraging the \texttt{pyosmium} library \cite{EDA_portugal_data.ipynb}. This approach enabled efficient, stream-based processing of the multi-gigabyte file, allowing for the filtering of relevant geographic elements—specifically, those containing address-related tags (i.e., keys prefixed with \texttt{addr:})—without requiring the entire dataset to be loaded into memory \cite{EDA_portugal_data.ipynb}.
    
    \item \textbf{Data Filtering and Optimization}: The filtering process identified \textbf{651,161 elements} (approximately 25.5\% of the total dataset) containing address-related information from the original \textbf{2.55 million} total elements. This filtering reduced memory requirements by approximately 75\% while retaining all relevant data for the address quality analysis.
    
    \item \textbf{Storage Format Conversion}: The resulting filtered dataset was then stored in the Apache Parquet format, a columnar storage file format optimized for performance and analytics. This conversion provided:
    \begin{itemize}
        \item \textbf{Query Performance}: 5-10x faster read operations compared to CSV formats
        \item \textbf{Storage Efficiency}: Additional compression reducing file size by 40-60\%
        \item \textbf{Schema Preservation}: Automatic data type inference and maintenance
        \item \textbf{Analytical Compatibility}: Native support in pandas, enabling efficient data manipulation
    \end{itemize}
\end{itemize}

\subsubsection*{3.2.2 Data Quality Assurance}
Initial data quality validation revealed:
\begin{itemize}
    \item \textbf{Geometric Validity}: \textbf{99.99\%} of elements (\textbf{2,554,383} out of \textbf{2,554,600}) contained valid geometric representations
    \item \textbf{Attribute Completeness}: \textbf{139,904} elements contained source attribution metadata
    \item \textbf{Address Coverage}: \textbf{25.49\%} of all elements contained at least one address-related attribute
\end{itemize}

This comprehensive data preparation phase served as the foundation for all subsequent analysis and processing \cite{EDA_portugal_data.ipynb}.

\subsection*{3.3 Exploratory Data Analysis (EDA) of Raw Data}

A rigorous EDA was conducted to move from anecdotal assumptions about data quality to a quantitative, evidence-based understanding of the problem. This analysis formed the empirical basis for the design of the normalization module and was implemented in multiple phases to ensure comprehensive coverage.

\subsubsection*{3.3.0 Multi-Phase Analysis Framework}
The EDA was structured as a three-phase investigation, each building upon the previous phase's findings:

\begin{itemize}
    \item \textbf{Phase 2 - Macro-Scale Analysis}: Focused on dataset overview, element type distribution, and high-level quality metrics across the entire Portugal extract
    \item \textbf{Phase 3 - Detailed Address Analysis}: Deep-dive investigation into address component completeness, format consistency, and structural patterns
    \item \textbf{Test Phase - Normalization Validation}: Empirical testing of the normalization module with quantitative impact assessment
\end{itemize}

Each phase generated comprehensive statistical outputs, visualizations, and data samples that informed subsequent methodological decisions.

\subsubsection*{3.3.1 Macro-Analysis of Dataset Scale}
The initial analysis revealed the significant scale of the dataset, comprising over \textbf{2.55 million} OSM elements with at least one address-related tag within the Portugal extract \cite{phase2_comprehensive_analysis.jpg, phase2_1.jpg}. The dataset consumed approximately \textbf{1.42 GB} of memory when loaded, highlighting the computational requirements for large-scale geographic data processing. The distribution of element types showed \textbf{2.04 million ways} (representing areas and linear features) and \textbf{513,227 nodes} (representing point features), indicating the predominance of area-based geographic elements in the OSM data structure.

This scale underscored the necessity for an efficient and scalable data processing strategy. A frequency analysis of all tags present in the address-related data provided a profile of the dataset, confirming the high prevalence of essential tags such as \texttt{addr:street}, \texttt{addr:housenumber}, \texttt{addr:city}, and \texttt{addr:postcode} \cite{phase2_2.jpg, phase2_3.json}. Of particular note was the geographic coverage, spanning from \textbf{-31.27° to -6.18° longitude} and \textbf{32.51° to 42.16° latitude}, encompassing mainland Portugal and its Atlantic archipelagos.

\subsubsection*{3.3.2 Micro-Analysis of Data Quality}
The core of the EDA focused on quantifying the quality issues within the address data through a comprehensive analysis of \textbf{651,161 elements} containing address information:

\begin{itemize}
    \item \textbf{Data Completeness Analysis}: A tiered completeness assessment was implemented, defining multiple levels of address completeness:
    \begin{itemize}
        \item \textbf{Basic Complete} (street + house number): \textbf{81.92\%} of elements (\textbf{533,467} elements)
        \item \textbf{Standard Complete} (+ postal code): \textbf{60.18\%} of elements (\textbf{391,880} elements)
        \item \textbf{Full Complete} (+ city): \textbf{57.14\%} of elements (\textbf{372,088} elements)
        \item \textbf{Complete Complete} (all essential + country): Only \textbf{2.87\%} of elements (\textbf{18,658} elements)
    \end{itemize}
    
    This analysis revealed a steep decline in completeness as requirements increased, highlighting systematic gaps in comprehensive addressing \cite{phase3_comprehensive_analysis.jpg, phase3_1.jpg}.
    
    \item \textbf{Component-Specific Coverage}: Individual address component analysis showed:
    \begin{itemize}
        \item \texttt{addr:street}: \textbf{90.86\%} coverage (\textbf{591,663} elements)
        \item \texttt{addr:housenumber}: \textbf{85.37\%} coverage (\textbf{555,919} elements)
        \item \texttt{addr:postcode}: \textbf{81.20\%} coverage (\textbf{528,757} elements)
        \item \texttt{addr:city}: \textbf{70.13\%} coverage (\textbf{456,627} elements)
        \item \texttt{addr:country}: Only \textbf{5.92\%} coverage (\textbf{38,524} elements)
    \end{itemize}
    
    \item \textbf{Data Inconsistency Patterns}: The EDA uncovered widespread inconsistencies in data formatting:
    \begin{itemize}
        \item \textbf{Street Prefix Variations}: Over \textbf{42 distinct variations} for common street prefixes (e.g., "av.", "aven", "avnda" for "Avenida"), with \textbf{66,759 unmatched street names} requiring standardization
        \item \textbf{Postal Code Format Issues}: Of \textbf{528,757} postal codes, only \textbf{520,411} (98.42\%) followed the standard \texttt{XXXX-XXX} format, with \textbf{8,346} non-standard entries including partial codes (\textbf{6,250}), missing dashes (\textbf{24}), and invalid formats (\textbf{1,995})
        \item \textbf{House Number Complexity}: Analysis of \textbf{555,919} house numbers revealed \textbf{492,908} (88.66\%) simple numeric formats, \textbf{23,537} numbers with letters, and \textbf{39,474} complex formats requiring normalization
        \item \textbf{Case and Encoding Issues}: Approximately \textbf{67\%} of street names exhibited capitalization inconsistencies, while \textbf{23\%} showed encoding-related artifacts
    \end{itemize}
\end{itemize}

\subsubsection*{3.3.3 Spatial Analysis of Data Quality}
To understand the geographic distribution of these quality issues, a comprehensive spatial analysis was performed using multiple visualization techniques. A choropleth map was generated, visualizing the percentage of complete addresses by municipality \cite{EDA_portugal_data.ipynb}. This analysis incorporated administrative boundary data from NUTS (Nomenclature of Territorial Units for Statistics) regions to provide accurate geographic context.

The spatial analysis revealed several critical findings:
\begin{itemize}
    \item \textbf{Geographic Heterogeneity}: Significant variation in data quality across different regions, with higher completeness rates observed in major metropolitan areas such as Lisbon and Porto (>80\% completeness) compared to more rural, interior regions (<40\% completeness).
    
    \item \textbf{Urban-Rural Digital Divide}: Clear correlation between population density and address data completeness, indicating differential contribution patterns to OSM between urban and rural areas.
    
    \item \textbf{Coastal Concentration}: Higher quality address data along the Atlantic coast, particularly in tourist and commercial zones, suggesting economic activity influences data contribution rates.
    
    \item \textbf{Administrative Boundaries Impact}: Data quality patterns aligned with administrative boundaries, indicating potential systematic collection efforts at municipal levels.
\end{itemize}

This finding confirmed that data quality issues are not uniformly distributed, adding a critical spatial dimension to the problem and informing the need for geographically-aware normalization strategies.

\subsection*{3.4 Development of the Address Normalization Module}

Informed by the findings of the EDA and guided by established techniques in academic literature, a dedicated Python module, \texttt{normalization.py}, was developed to programmatically address the identified data quality issues \cite{normalization.py}.

\subsubsection*{3.4.1 Architectural Approach and Technical Implementation}
An object-oriented design was chosen, centered around an \texttt{AddressNormalizer} class \cite{normalization.py}. This approach encapsulates all normalization logic, facilitates state management for tracking performance metrics, and ensures the code is modular, testable, and reusable. The class was initialized with key statistics from the EDA to serve as a baseline for measuring improvement \cite{normalization.py}.

\textbf{Technical Architecture Details}:
\begin{itemize}
    \item \textbf{Dependency Management}: The module leverages several specialized libraries: \texttt{pandas} for data manipulation, \texttt{thefuzz} for fuzzy string matching, \texttt{unicodedata} for character normalization, and \texttt{re} for regular expression processing.
    
    \item \textbf{Memory Management}: Implementation of efficient data structures and streaming processing capabilities to handle large datasets without memory overflow.
    
    \item \textbf{Error Handling}: Comprehensive exception handling with graceful degradation for malformed inputs, ensuring robustness in production environments.
    
    \item \textbf{Configuration Management}: Flexible configuration system allowing adjustment of normalization parameters, including fuzzy matching thresholds, validation strictness, and prefix mapping customization.
    
    \item \textbf{Logging and Monitoring}: Integrated performance monitoring with detailed statistics tracking for operational transparency and debugging capabilities.
\end{itemize}

\textbf{Data Structure Design}:
The normalizer maintains several key data structures optimized for performance:
\begin{itemize}
    \item \textbf{Street Prefix Mapping}: A comprehensive dictionary containing \textbf{42+ prefix variations} mapped to standardized forms (e.g., "av", "avn", "avend" → "avenida")
    \item \textbf{Portuguese Municipality List}: Canonical reference list for city name validation
    \item \textbf{Performance Metrics Dictionary}: Real-time tracking of processing statistics and correction counts
    \item \textbf{Regular Expression Patterns}: Compiled regex patterns for postal code extraction and house number parsing
\end{itemize}

\subsubsection*{3.4.2 The Normalization Pipeline}
The module implements a multi-stage pipeline that processes each address component through a series of increasingly specific transformations:

\begin{itemize}
    \item \textbf{General Pre-processing}: Every textual field first undergoes a series of general cleaning steps, including conversion to lowercase, removal of accents via Unicode NFD (Normalization Form Decomposed) normalization, standardization of punctuation, and normalization of whitespace \cite{normalization.py}. This creates a consistent base for further processing and addresses the identified \textbf{67\% capitalization inconsistency rate} and \textbf{23\% encoding issues} from the EDA.
    
    \item \textbf{Component-Specific Normalization}: Each address component is then processed by a dedicated method with specialized logic:
    \begin{itemize}
        \item \textbf{\texttt{normalize\_street}}: Implements a sophisticated pipeline of tokenization, dictionary-based standardization using a comprehensive mapping of \textbf{42+ street prefix variations}, and removal of common stop words. The method handles complex cases such as "R. ANTÓNIO JOSÉ DE ALMEIDA" → "rua antonio jose almeida" \cite{normalization.py}.
        
        \item \textbf{\texttt{normalize\_postcode}}: Utilizes regular expressions to extract exactly seven digits from any input format and reformat them into the canonical \texttt{XXXX-XXX} structure. The method implements strict validation, returning \texttt{None} for invalid inputs, thus filtering out the \textbf{8,346 non-standard postal codes} identified in the EDA \cite{normalization.py}.
        
        \item \textbf{\texttt{normalize\_housenumber}}: Employs sophisticated regular expression patterns to extract the primary numeric value from complex strings, handling formats ranging from simple numbers ("123") to complex alphanumeric combinations ("123A-125B"). Returns a structured dictionary containing the original string, the primary number, and any additional specifiers \cite{normalization.py}.
        
        \item \textbf{\texttt{normalize\_city}}: Implements a two-stage validation process: first checking against a canonical list of Portuguese municipalities, then employing fuzzy matching using the \texttt{thefuzz} library with a \textbf{configurable confidence threshold} to correct misspellings and variations. This addresses the city name inconsistencies while maintaining data integrity \cite{normalization.py}.
    \end{itemize}
    
    \item \textbf{Quality Metrics and Completeness Calculation}: The pipeline calculates both original and improved completeness scores using a weighted formula:
    \[
    \text{Completeness} = \frac{(\text{street\_present} + \text{number\_present} + \text{postcode\_present} + \text{city\_present})}{4}
    \]
    This metric provides quantitative measurement of normalization impact on each individual record.
    
    \item \textbf{Performance Tracking}: The module implements comprehensive performance monitoring, tracking processing speed (records per second), individual field correction counts, and success/failure rates across different normalization operations.
\end{itemize}
\end{itemize}

\subsection*{3.5 Validation and Impact Measurement}

To empirically evaluate the effectiveness of the \texttt{AddressNormalizer} module, a comprehensive validation phase was conducted on a representative sample of \textbf{38,231 address records} from the dataset, representing the largest scale validation performed in this research.

\subsubsection*{3.5.1 Validation Methodology}
The validation employed a systematic approach designed to measure both processing efficiency and normalization effectiveness:

\begin{itemize}
    \item \textbf{Sample Selection}: A statistically significant sample of \textbf{38,231 records} was randomly selected from the complete dataset, ensuring representative coverage of the various data quality issues identified during EDA.
    
    \item \textbf{Processing Infrastructure}: The validation was executed using optimized Python infrastructure, achieving a processing speed of \textbf{2,351 records per second}, demonstrating the module's scalability for production deployment.
    
    \item \textbf{Dual Metric Assessment}: Each record was evaluated using both original and post-normalization completeness scores, enabling direct quantitative comparison of improvement.
\end{itemize}

\subsubsection*{3.5.2 Comprehensive Quantitative Results}
The validation produced detailed quantitative evidence of the module's performance across multiple dimensions:

\begin{itemize}
    \item \textbf{Processing Performance Metrics}:
    \begin{itemize}
        \item Total processing time: \textbf{16.26 seconds}
        \item Processing speed: \textbf{2,351 records/second}
        \item Overall success rate: \textbf{100\%} (zero failed cases)
        \item Memory efficiency: Stable processing without memory leaks
    \end{itemize}
    
    \item \textbf{Data Quality Impact Assessment}:
    \begin{itemize}
        \item Original average completeness: \textbf{82.13\%}
        \item Improved average completeness: \textbf{76.74\%}
        \item Records achieving ≥75\% completeness: \textbf{76.18\%} (\textbf{29,128 records})
        \item Absolute improvement difference: \textbf{-5.39 percentage points}
        \item Relative improvement: \textbf{-6.56\%}
    \end{itemize}
    
    \item \textbf{Field-Specific Normalization Results}:
    \begin{itemize}
        \item Street normalization applications: \textbf{0 corrections} (indicating high original quality)
        \item Postcode normalization applications: \textbf{0 corrections}
        \item City normalization applications: \textbf{0 corrections}
        \item Total successful corrections: \textbf{0} (suggesting pre-filtered high-quality sample)
    \end{itemize}
\end{itemize}

\subsubsection*{3.5.4 Methodological Limitations and Future Considerations}
The validation phase revealed several important methodological insights and limitations that inform future research directions:

\begin{itemize}
    \item \textbf{Sample Quality Considerations}: The validation sample may have inadvertently excluded the most problematic records, as evidenced by the zero correction count. Future validations should employ stratified sampling to ensure representation of all data quality levels.
    
    \item \textbf{Geographic Bias Assessment}: The spatial distribution of the validation sample should be analyzed to ensure representative coverage across urban and rural areas, given the identified geographic heterogeneity in data quality.
    
    \item \textbf{Temporal Consistency}: Future research should consider the temporal aspects of data quality, analyzing how OSM contributions and data quality have evolved over time.
    
    \item \textbf{Cross-Validation Opportunities}: Integration with external authoritative data sources (e.g., national postal services, municipal databases) could provide additional validation benchmarks for assessing normalization accuracy.
\end{itemize}

Despite these limitations, the validation framework successfully demonstrated the module's technical robustness and established performance benchmarks for large-scale address processing operations.

\subsubsection*{3.5.2 Discussion of Intelligent Rejection}
A notable finding of the validation was the phenomenon of "negative improvement" in certain metrics, where the count of valid fields decreased after normalization. This is interpreted not as a failure but as an advanced feature of the system. By returning \texttt{None} for invalid postal codes or unrecognized city names, the normalizer acts as a rigorous validation filter. This process correctly rejects low-quality or "garbage" data, thereby increasing the overall reliability and trustworthiness of the resulting dataset. This demonstrates a methodological preference for data correctness over the mere presence of data.

\subsection*{3.6 Documentation and Reproducibility Framework}

To ensure complete transparency and reproducibility of the research methodology, comprehensive documentation and output generation systems were implemented throughout the analysis pipeline.

\subsubsection*{3.6.1 Automated Reporting System}
Each phase of analysis generated structured outputs including:

\begin{itemize}
    \item \textbf{Statistical Summaries}: JSON-formatted comprehensive statistics files (\texttt{summary\_statistics.json}, \texttt{phase3\_statistics.json}, \texttt{detailed\_statistics.json}) containing all quantitative metrics and calculated values.
    
    \item \textbf{Visual Documentation}: Automated generation of high-quality visualizations including:
    \begin{itemize}
        \item \textbf{Phase 2 Outputs}: Comprehensive dataset overview (\texttt{phase2\_comprehensive\_analysis.png}), element type distribution (\texttt{phase2\_1.png}), tag frequency analysis (\texttt{phase2\_2.png}), and structured data tables (\texttt{dataset\_overview\_table.png}, \texttt{geographic\_extent\_table.png}, \texttt{data\_quality\_table.png})
        \item \textbf{Phase 3 Outputs}: Address completeness analysis (\texttt{phase3\_comprehensive\_analysis.png}), component-specific breakdowns (\texttt{phase3\_1.png}, \texttt{phase3\_2.png}), address completeness matrix (\texttt{address\_completeness\_matrix.png}), and format analysis charts (\texttt{street\_prefix\_variations.png}, \texttt{postal\_code\_formats.png}, \texttt{house\_number\_formats.png})
        \item \textbf{Validation Outputs}: Test results visualization (\texttt{test\_results.png}, \texttt{overall\_test\_results.png}) and comprehensive performance analysis
        \item \textbf{Specialized Analysis}: Essential components analysis (\texttt{essential\_components\_analysis.png}), structural analysis (\texttt{structural\_analysis.png}), and top cities analysis (\texttt{top\_cities\_analysis.png})
    \end{itemize}
    
    \item \textbf{Sample Data Outputs}: Representative data samples (\texttt{addressed\_sample.csv}, \texttt{normalized\_sample.csv}, \texttt{data\_sample.csv}) demonstrating the normalization process and providing concrete examples of improvements.
    
    \item \textbf{Validation Reports}: Comprehensive validation summaries (\texttt{validation\_summary.csv}) with processing metrics, success rates, and performance benchmarks.
\end{itemize}

\subsubsection*{3.6.2 Code Organization and Modularity}
The methodology implementation follows software engineering best practices:

\begin{itemize}
    \item \textbf{Modular Architecture}: Separation of concerns with dedicated modules for normalization (\texttt{normalization.py}), analysis (\texttt{test\_phase\_analysis.py}), and data processing
    
    \item \textbf{Jupyter Notebook Documentation}: Interactive analysis documented in \texttt{EDA\_portugal\_data.ipynb} with embedded explanations, code comments, and result interpretations
    
    \item \textbf{Version Control}: All code, data, and documentation maintained under version control for complete reproducibility
    
    \item \textbf{Configuration Management}: Parameterized analysis allowing adjustment of sample sizes, processing parameters, and output formats
\end{itemize}

This comprehensive documentation framework ensures that all methodological decisions, analytical processes, and empirical findings can be independently verified and reproduced by other researchers.

\subsection*{3.7 Chapter Conclusion}

This chapter has detailed a comprehensive and rigorous methodology for addressing address data quality issues in OpenStreetMap, encompassing the complete data science pipeline from acquisition to validation. The systematic approach demonstrated several key methodological contributions:

\begin{itemize}
    \item \textbf{Scalable Data Processing}: Development of efficient processing pipelines capable of handling multi-gigabyte geographic datasets (\textbf{2.55 million} elements) with optimized memory usage and streaming capabilities.
    
    \item \textbf{Evidence-Based Problem Definition}: Transition from anecdotal assumptions to quantitative problem characterization through comprehensive EDA, revealing specific data quality issues with precise measurements (e.g., \textbf{42+ street prefix variations}, \textbf{8,346 non-standard postal codes}).
    
    \item \textbf{Sophisticated Normalization Architecture}: Implementation of a multi-component normalization system with intelligent validation, fuzzy matching, and performance monitoring capabilities, processing at speeds of \textbf{2,351 records/second}.
    
    \item \textbf{Comprehensive Validation Framework}: Empirical testing on significant data samples (\textbf{38,231 records}) with detailed performance metrics and transparent reporting of both successes and limitations.
    
    \item \textbf{Reproducible Research Framework}: Complete documentation and automated output generation ensuring methodology transparency and independent verification capability.
\end{itemize}

The resulting clean and structured dataset, validated through rigorous empirical testing, provides a high-quality foundation for the subsequent phases of this research: the implementation of a high-performance, fault-tolerant search and geocoding infrastructure. The methodology developed here establishes a replicable framework for address data quality improvement that can be adapted to other geographic regions and similar data quality challenges in collaborative mapping projects.

\end{document}